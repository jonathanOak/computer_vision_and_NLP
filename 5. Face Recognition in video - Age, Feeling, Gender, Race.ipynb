{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube links for demonstration\n",
    "## https://youtu.be/UnDuDLv1LdY\n",
    "## https://youtu.be/wmbnkztWZU4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "haar = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('91_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s = load_model('100_feeling.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = load_model('98_age.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r = load_model('92_race.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detect(img, model, model_s, model_a, model_r):\n",
    "    faces = haar.detectMultiScale(img,1.5,5)\n",
    "    \n",
    "    for x,y,w,h in faces:\n",
    "        cropped = img[y:y+h, x:x+w]\n",
    "        np_image = np.array(cropped).astype('float32')/255\n",
    "\n",
    "        np_res = transform.resize(np_image, (200,200,3))\n",
    "\n",
    "        np_res = np.expand_dims(np_res, axis=0)\n",
    "\n",
    "\n",
    "        pred = model.predict(np_res)\n",
    "\n",
    "        prediction = pred[0][0]\n",
    "        \n",
    "        gender = ''\n",
    "        if prediction > 0.5:\n",
    "            gender = 'Female {:.2f}%'.format(prediction*100)\n",
    "        else:\n",
    "            gender = 'Male {:.2f}%'.format((1-prediction)*100)\n",
    "        #-----------------------------------------------------\n",
    "        np_res_s = transform.resize(np_image, (48,48,3))\n",
    "        np_res_s = np.expand_dims(np_res_s, axis=0)\n",
    "        pred = model_s.predict(np_res_s)\n",
    "        prediction = np.where(pred[0] == max(pred[0]))\n",
    "        prediction = prediction[0][0]\n",
    "        emotion = ''\n",
    "        if prediction == 0:\n",
    "            emotion = 'Anger' \n",
    "        elif prediction == 1:\n",
    "            emotion = 'Contempt'\n",
    "        elif prediction == 2:\n",
    "            emotion = 'Disgust'\n",
    "        elif prediction == 3:\n",
    "            emotion = 'Fear'\n",
    "        elif prediction == 4:\n",
    "            emotion = 'Happy'\n",
    "        elif prediction == 5:\n",
    "            emotion = 'Sadness'\n",
    "        elif prediction == 6:\n",
    "            emotion = 'Surprise'\n",
    "        #------------------------------------------------------\n",
    "        np_res_a = transform.resize(np_image, (64,64,3))\n",
    "        np_res_a = np.expand_dims(np_res_a, axis=0)\n",
    "\n",
    "        pred = model_a.predict(np_res_a)\n",
    "        prediction = np.where(pred[0] == max(pred[0]))\n",
    "        age = str(prediction[0][0])\n",
    "        #------------------------------------------------------\n",
    "        #np_image = transform.resize(np_image, (48,48,3))\n",
    "        #np_image = np.expand_dims(np_image, axis=0)\n",
    "        pred = model_r.predict(np_res_s)\n",
    "\n",
    "        prediction = np.where(pred[0] == max(pred[0]))\n",
    "        prediction = prediction[0][0]\n",
    "        race = ''\n",
    "        if prediction == 0:\n",
    "            race = 'White'\n",
    "        elif prediction == 1:\n",
    "            race = 'Black'\n",
    "        elif prediction == 2:\n",
    "            race = 'Asian'\n",
    "        elif prediction == 3:\n",
    "            race = 'Indian'\n",
    "        elif prediction == 4:\n",
    "            race = 'Other'\n",
    "\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        cv2.putText(img, gender, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "        cv2.putText(img, emotion, (x, y-40), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "        cv2.putText(img,'Age: ' + age, (x-110, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (237,225,141), 2)\n",
    "        cv2.putText(img,'Race: ' + race, (x-110, y+30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (237,225,141), 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('couples.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 25617 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A3899B1A68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "## videos are combination of features, they are continuous, like frames/second\n",
    "\n",
    "while True:\n",
    "    #ret(return) = as long as there is a video it will be true\n",
    "    ret,frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    \n",
    "    #passing every frame through the function\n",
    "    frame = face_detect(frame, model, model_s, model_a, model_r)\n",
    "    cv2.imshow('object_detect', frame)\n",
    "\n",
    "    if cv2.waitKey(40) == 27:\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
